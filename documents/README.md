# RAG-enabled Business Rule Management

This application demonstrates the use of Retrieval Augmented Generation (RAG) to enhance the capabilities of AI agents in translating natural language business rules into structured logic and code.

## Features

- **Agent 1**: Extracts structured conditions and actions from natural language descriptions, enhanced with RAG to provide business context
- **Agent 2**: Translates structured rule representations into Drools Rule Language (DRL), enhanced with RAG to learn from existing rules
- **Local Document Knowledge Base**: Stores business documents and rules for both agents
- **Remote Document Retrieval**: Can fetch documents from remote sources (configurable)

## Setup

1. Clone the repository
2. Install the required dependencies:
   ```
   pip install -r gemini-gradio-poc/requirements.txt
   ```
3. Set your Google API key in a `.env` file:
   ```
   GOOGLE_API_KEY=your_api_key_here
   ```
4. Run the application:
   ```
   python run_gradio_ui.py
   ```

## Adding Documents to the Knowledge Base

### For Agent 1 (Business Rules Context)

Place your business documents in the `documents/agent1/` folder. Supported formats:
- Microsoft Word (.docx)
- PDF (.pdf)

These documents should contain business rules, policies, or other relevant information that helps the AI understand the context of the business rules.

### For Agent 2 (Rule Generation Learning)

Place example DRL files in the `documents/agent2/` folder. These should have the `.drl` extension and follow the Drools Rule Language syntax.

Example:
```
rule "Restaurant_Size_Small"
when
    Restaurant( size == "Small" )
    Forecast( total_sales >= 0, total_sales < 50 )
then
    Staffing.base_employees = 2;
end
```

## Generated Rules

All rules generated by the system are stored in the `documents/drl_files/` folder. These can be used to:
1. Add to the Agent 2 knowledge base (by moving them to `documents/agent2/`)
2. Import into a Drools rules engine
3. Review and audit rule changes

## How RAG Works in this Application

1. **Document Preparation**:
   - Documents are read and split into smaller chunks
   - Each chunk is converted into an embedding (vector representation)
   - These embeddings are stored in memory during the application runtime

2. **When a User Submits a Query**:
   - The query is converted into an embedding
   - The system finds the most similar document chunks using cosine similarity
   - These relevant chunks are provided as context to the AI model

3. **Enhanced Response Generation**:
   - The AI model (Gemini) generates a response based on the query and the provided context
   - This combines the model's general knowledge with specific information from your documents
   - The result is more accurate, contextualized, and aligned with your business rules

## Customization

The system can be customized by modifying the following parameters in `rag.py`:
- `chunk_size`: Size of document chunks (default: 500)
- `chunk_overlap`: Overlap between chunks (default: 50)
- `AGENT1_SYSTEM_PROMPT`: System prompt for Agent 1
- `AGENT2_SYSTEM_PROMPT`: System prompt for Agent 2